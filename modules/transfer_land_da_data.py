# Module for extracting data from source.
from get_land_da_data import GetSrwData
from progress_bar import ProgressPercentage
from upload_data import UploadData


class TransferLandDaData():
    """
    Obtain directories for the datasets tracked by the data tracker bot.
    
    """
    def __init__(self, linked_home_dir, platform="orion"):
        """
        Args: 
             linked_home_dir (str): User directory linked to the RDHPCS' root
                                    data directory.
             platform (str): RDHPCS of where the datasets will be sourced.
        """
    
        # Establish locality of where the dataseta will be sourced.
        self.linked_home_dir = linked_home_dir
        if platform == "orion":
            self.fix_data_dir = self.linked_home_dir + "/noaa/fv3-cam/UFS_Land_Da_App/develop/fix.tar"
            self.input_model_data_dir = linked_home_dir + "/noaa/fv3-cam/UFS_Land_Da_App/develop/input_model_data.tar"
        else:
            print("Select a different platform.")
            
        # Instantiate Land DA uploader
        self.land_da_uploader = GetLandDaData(None, None, self.fix_data_dir, self.input_model_data_dir)

        # List all data directories from sources (filtered)
        print("\nExtracting list of data directories from sources (filtered)...")
        self.ma_data_list = self.land_da_uploader.ma_data_list
        self.fix_data_list = self.land_da_uploader.fix_data_list

        # Land DA input model analysis & fixed data file locations (filtered)
        print("Extracting list of data directories from work directory (filtered)...")
        self.land_da_ma_data_dirs = self.land_da_uploader.ma_file_dirs
        self.land_da_fix_data_dirs = self.land_da_uploader.fix_file_dirs

        # Select model analysis files based on external model it was generated by (filtered)
        print("Partitioning data directories from work directory (filtered) into categories ...")
        self.land_da_ma_dict = self.land_da_uploader.partition_ma_datasets 
        self.land_da_fix_dict = self.land_da_uploader.partition_fixed_datasets 
        print("\033[1m" + f"\nLand DA MA data:" + "\033[0m" + f"\n{self.land_da_ma_dict}")    
        print("\033[1m" + f"\nLand DA Fix data:" + "\033[0m" + f"\n{self.land_da_fix_dict}")
    
        # Upload fixed & input model data.
        #UploadData(land_da_fix_dict, use_bucket='srw').upload_files2cloud()
        #UploadData(land_da_ma_dict, use_bucket='srw').upload_files2cloud()        
        print("\033[1m" + f"\nLand DA Fix & MA data transfer to S3 bucket complete." + "\033[0m") 
        
        
if __name__ == '__main__': 
    
    # Obtain directories & upload to cloud for all the fix and model input Land DA datasets
    land_da_xfer = TransferLandDaData(linked_home_dir="/home/schin/work", platform="orion")
